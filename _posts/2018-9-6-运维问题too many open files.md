---

layout: post

title: Linux服务器的Too many open files问题

date: 2018-9-6

tags: 运维

---
### 现象
- 昨天，前端同学请求API接口，有时候成功，有时候又失败，偶发性的问题，不好排查。一开始，初步判定为是Jenkins集成时导致服务器重启，从而前端请求时出现错误，没有正确认识到问题所在。
- 今天早上到公司后，觉得不对经，错误也需要经历才是一种经验，遂排查之。
- 通过vim指令打开昨天的日志
	- `:8000`该指令表示跳转到第8000行
	- 向前滚动一屏：Ctrl+F
	- 向后滚动一屏：Ctrl+B
	- 向前滚动半屏：Ctrl+D（向下）
	- 向后滚动半屏：Ctrl+U（向上）
	- 向下滚动一行，保持当前光标不动：Ctrl+E
	- 向上滚动一行，保持当前光标不动：Ctrl+Y
- 满屏都是`java.io.IOException: Too many open files`或者`java.net.SocketException`，这两个错误

### 排查
- 搜索引擎上找到了问题排查思路，Linux的进程打开文件句柄限制
> too many open files(打开的文件过多)是Linux系统中常见的错误，从字面意思上看就是说程序打开的文件数过多，不过这里的files不单是文件的意思，也包括打开的通讯链接(比如socket)，正在监听的端口等等，所以有时候也可以叫做句柄(handle)，这个错误通常也可以叫做句柄数超出系统限制。

- 通过`ulimit -a`查看当前系统设置的最大句柄数是多少，如果没有修改过，默认将是1024
- 通过jps查看当前Linux服务器上运行的java程序，我看到大概有十个左右
- 使用命令`lsof -p 进程id | wc -l`可以统计进程打开了多少个文件
	- 例如：`lsof -p 3092 | wc -l`，可以得到3092这个进程当前打开的文件
- 使用上述命令，查看到进程id为23623的进程打开了876个文件
- 通过`ps -ef | grep 23623`指令查看，发现是一个名为skywalking的项目，这是其他同事部署的一个监控项目
> SkyWalking 创建与2015年，提供分布式追踪功能。从5.x开始，项目进化为一个完成功能的Application Performance Management系统。它被用于追踪、监控和诊断分布式系统，特别是使用微服务架构，云原生或容积技术。

- anyway，这个项目是同事部署的实验性项目，并没有太多作用，但它占用了服务器可以打开文件句柄三分之二的量，这显然不合理

### 解决
- 解决方案从两方面着手，“开源节流”
- 开源：修改服务器可以打开文件句柄的最大数量
	- 通过指令`ulimit -n 2048`，这样就可以把当前用户的最大允许打开文件数量设置为2048了，但这种设置方法在***重启后会还原为默认值***。ulimit -n命令非root用户只能设置到4096。想要设置到8192需要sudo权限或者root用户。
- 节流：把上一步排查到的那个进程id为23623的进程杀掉
	- 通过指令，`kill -9 23623`将进程杀掉
	
	
	
### 后记
#### 2018-11-27
最近一段时间，这个问题再次出现，这一次日志文件里面看不出任何有用的信息。原本打算去看堆栈信息，但是QA赶进度，没办法长时间保留现场，只得重启，留下一堆日志文件分析。看了很久日志文件，发现一个jedis的错误。redis-client的默认连接
是10000，而这台服务器的最大文件句柄数是10240非常接近。后面尝试了几个可能触发的接口，发现登录这个接口在每一次登录的时候，都会给当前进程的文件句柄数+10左右，这意味着用不了多久就会达到10000，一旦达到最大值，加之进程内其他线程
可能操作文件增加的文件句柄数，很容易就可以达到10240，从而出现Too many open files。目前初步排查出的问题就出在这里，但是不是非常确定，找运维的同事去确认了生产上的服务器的配置，确认不可能出现同类问题，暂时放心了。如果想要更精
确的查找到问题所在，还是应该从堆栈信息入手，去分析。但是目前QA赶进度，所以只能暂时先将进程最大文件句柄数设为65535试试看。如果再次出现同类问题，就只能证明定位问题不准确，但是如果问题不再出现，那问题可能就是这里。