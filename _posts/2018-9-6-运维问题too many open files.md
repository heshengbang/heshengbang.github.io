---

layout: post

title: Linux服务器的Too many open files问题

date: 2018-9-6

tags: 运维

---
### 现象
- 昨天，前端同学请求API接口，有时候成功，有时候又失败，偶发性的问题，不好排查。一开始，初步判定为是Jenkins集成时导致服务器重启，从而前端请求时出现错误，没有正确认识到问题所在。
- 今天早上到公司后，觉得不对经，错误也需要经历才是一种经验，遂排查之。
- 通过vim指令打开昨天的日志
	- `:8000`该指令表示跳转到第8000行
	- 向前滚动一屏：Ctrl+F
	- 向后滚动一屏：Ctrl+B
	- 向前滚动半屏：Ctrl+D（向下）
	- 向后滚动半屏：Ctrl+U（向上）
	- 向下滚动一行，保持当前光标不动：Ctrl+E
	- 向上滚动一行，保持当前光标不动：Ctrl+Y
- 满屏都是`java.io.IOException: Too many open files`或者`java.net.SocketException`，这两个错误

### 排查
- 搜索引擎上找到了问题排查思路，Linux的进程打开文件句柄限制
> too many open files(打开的文件过多)是Linux系统中常见的错误，从字面意思上看就是说程序打开的文件数过多，不过这里的files不单是文件的意思，也包括打开的通讯链接(比如socket)，正在监听的端口等等，所以有时候也可以叫做句柄(handle)，这个错误通常也可以叫做句柄数超出系统限制。

- 通过`ulimit -a`查看当前系统设置的最大句柄数是多少，如果没有修改过，默认将是1024
- 通过jps查看当前Linux服务器上运行的java程序，我看到大概有十个左右
- 使用命令`lsof -p 进程id | wc -l`可以统计进程打开了多少个文件
	- 例如：`lsof -p 3092 | wc -l`，可以得到3092这个进程当前打开的文件
- 使用上述命令，查看到进程id为23623的进程打开了876个文件
- 通过`ps -ef | grep 23623`指令查看，发现是一个名为skywalking的项目，这是其他同事部署的一个监控项目
> SkyWalking 创建与2015年，提供分布式追踪功能。从5.x开始，项目进化为一个完成功能的Application Performance Management系统。它被用于追踪、监控和诊断分布式系统，特别是使用微服务架构，云原生或容积技术。

- anyway，这个项目是同事部署的实验性项目，并没有太多作用，但它占用了服务器可以打开文件句柄三分之二的量，这显然不合理

### 解决
- 解决方案从两方面着手，“开源节流”
- 开源：修改服务器可以打开文件句柄的最大数量
	- 通过指令`ulimit -n 2048`，这样就可以把当前用户的最大允许打开文件数量设置为2048了，但这种设置方法在***重启后会还原为默认值***。ulimit -n命令非root用户只能设置到4096。想要设置到8192需要sudo权限或者root用户。
- 节流：把上一步排查到的那个进程id为23623的进程杀掉
	- 通过指令，`kill -9 23623`将进程杀掉